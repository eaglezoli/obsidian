### Aim & Relevance

The paper sets out to **raise student motivation and perceived relevance in a biomedical signals-and-systems course by embedding an “experiential learning space” built around commercial wearable devices and cloud-based coding notebooks.** This objective is timely: biomedical engineering programmes worldwide struggle to translate theory into practice early in the curriculum. The study therefore targets a genuine pedagogical pain-point in BME education and aligns with current accreditation pressures to deliver evidence of competency-based learning. ([mdpi.com][1])

### Methodological Strengths

* **Mixed active-learning ecology.** The authors deliberately layer direct instruction, challenge-based learning, and Kolb-mapped experiential tasks, demonstrating thoughtful instructional design rather than a one-off “activity add-on.” ([mdpi.com][1])
* **Quasi-experimental comparison.** A control cohort used the same data-analysis notebook with public ECG data, while the experimental cohort captured their own signals with the wearables. This at least partially isolates the contribution of authentic data capture. ([mdpi.com][1])
* **Use of validated instruments.** Student Assessment of Learning Gains (SALG) and a programme‐specific rubric offer recognised measures for attitudes and competency. Statistical analysis with non-parametric tests is appropriate given small-sample ordinal data. ([mdpi.com][1])
* **Transparent artefacts.** The provision of open-source Python notebooks and explicit mapping to curricular outcomes enhances potential replicability.

### Limitations & Weaknesses

| Aspect                | Critique                                                                                                                                                                              |
| --------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Research design**   | *Non-random assignment* and a single-institution setting weaken internal validity and leave the results vulnerable to cohort and instructor effects.                                  |
| **Sample & duration** | N ≈ 46 over a five-week window limits statistical power; most differences remained non-significant, yet the discussion occasionally implies practical superiority.                    |
| **Outcome metrics**   | Reliance on self-reported relevance/interest/motivation and one written exam offers little evidence of deep conceptual gain or long-term skill transfer.                              |
| **Data analysis**     | Focus on dispersion changes (IQR drop) is interesting but under-theorised; effect sizes are not reported, masking the practical magnitude of any change.                              |
| **Theoretical depth** | Kolb’s cycle is invoked but not problematised; alternative constructs (situational interest, expectancy-value, self-determination) are cited yet not operationalised in the analysis. |
| **Cost-benefit**      | Although the authors flag resource demands, there is no economic or logistical analysis—critical for scaling in resource-constrained programmes. ([mdpi.com][1])                      |

### Engagement with Pedagogical Theory

The paper positions itself in experiential and challenge-based learning traditions and cites relevance/interest literature; however, theoretical engagement is largely **descriptive rather than analytic**. Kolb’s cycle is used as a post-hoc mapping of activities, not a driver for data interpretation. There is minimal dialogue with recent critical literature on experiential learning’s limitations or on the nuances of student motivation in engineering contexts. ([mdpi.com][1])

### Contribution to the Field

* **Novelty**: Integrating consumer-grade wearables into an early-stage BME signals course is fresh and practical.
* **Practicality**: Detailed activity timelines, open notebooks, and explicit alignment with an institutional competency rubric aid local adoption.
* **Replicability**: The intervention is reproducible in settings with similar access to wearables; however, scalability beyond well-resourced institutions is uncertain.
* **Impact**: Given the largely non-significant quantitative gains, the work is best read as a *design exemplar* rather than definitive evidence of effectiveness.

### Writing & Scholarly Quality

The manuscript is **well-structured and thoroughly referenced**, but portions of the introduction and theoretical framework are lengthy and occasionally repetitive. Results are clearly tabulated, yet the prose sometimes over-interprets non-significant findings (e.g., “positive impact” claims based on reduced variability alone). Minor language issues (e.g., tense shifts) do not obscure meaning but detract from polish.

### Overall Evaluation

This case study is a **constructively ambitious attempt** to make biomedical signals education tangible through wearable technology and cloud collaboration. As evidence, it is preliminary: small-scale, short-term, and attitudinally focused. Its real value lies in the **detailed blueprint for instructors** seeking to incorporate authentic data capture into BME curricula. Future work should:

1. Employ randomised or crossover designs to mitigate cohort bias.
2. Track longitudinal knowledge retention and actual competency performance.
3. Include cost-effectiveness and equity analyses for broader adoption.
4. Engage more critically with motivational theory to explain *why* certain students respond differently to experiential tech-enhanced tasks.

Used judiciously, the paper offers **actionable design ideas** but should not yet be treated as conclusive proof of learning-outcome enhancement.

[1]: https://www.mdpi.com/2227-7102/12/9/598 "Experiential Learning in Biomedical Engineering Education Using Wearable Devices: A Case Study in a Biomedical Signals and Systems Analysis Course"
